z-score: Measurment to tell how many standard deviations a data point is off the mean.
p-value:



Ziel eines guten Watermarks:
    - Zuverlässige Detection eines gewatermarkten Textes
        - Selbst bei Veränderungen des Textes oder anaylse von Teilen desen
    - Geringe veränderung des orginal Textes
    - Integrität des Textes 
        - Das Watermark sollte nicht einfach zu erkennen, verändern oder zu fälschen sein
    - Informationen über das Watermark
        - Codierung von Informationen zur Rückschließung auf Informationen
            - Author / Organisation
            - Model
            - Nutzungszweck

Frage: - Wie kann man effezient Informationen in einem Text embedden, sodass diese die obere Kriterien größtmöglich erfüllt
        - Wie kann dies mit bereits genierten Texten gemacht werden

Linguistische Watermarks: gezielte Variation von Synonymen, Satzlängen, Phrasenhäufigkeiten (statistisch erkennbar).

Kryptografische Signaturen: Hash des Textes + privater Schlüssel → später per öffentlichem Schlüssel überprüfbar.

Metadaten-basierte Watermarks: Einbettung in HTML, PDF oder DOCX-Metadaten.

Steganografische Watermarks: Unsichtbare Zeichen (z. B. Unicode Zero Width Spaces) – leicht, aber anfällig für Entfernung.

Ein gutes Watermark balanciert:

    - Robustheit ↔ Unauffälligkeit

    - Einfache Überprüfbarkeit ↔ Fälschungssicherheit

    - Transparenz ↔ Datenschutz


H. Zhang et al. (2025): Formalisierung als Hypothesentest mit klarer Aussage:
→ Jedes Watermark hat ein Dilemma:

    Entweder stark genug, um erkannt zu werden → leicht entfernbar,
    oder schwach genug, um unauffällig zu bleiben → statistisch nicht nachweisbar.



MOMENTANE FRAGEN: Wie viele DoubleWP braucht man pro Character um eine Sucess vom Detekten von über 90% bekommt.


Idee: Logits die Werte sehr nah beiander sind -> Synonymen ?? 
    - Wenn ja könnte durch wahl des ersten oder anderen eventuell ein binärer Code eingebettet werden, jedoch muss immer das gleiche Modell benutzt werden
        -Unwahrscheinlich, gleiche Logits sind nicht Synonyme
    - Veränderung des Textes um semantische Intakthaltung zu erhalten cosine_similarity > 0.9



Ergebnisse:
    Gamma = 1:
    -> ROC AUC ~ 0.54

    Autoregression angepasst 
    - Mit einem Datensatz von ~400 
    - Gamma=3, Green_frac=.5
    -> AUC ROC ~ 0.97



TODO
-implementiere zweite Funktion
    + Addieren des Semantischen Emebdding Spaces
    + Verändere Tokens entsprechend starker Entropie Räume
    + Lasse Anfange unerändert (Von vorteil für Semantisches Model und Logik)

- Code Aufräumen
- Mehr auf mich anpassen
- Stellen kommentieren
- Schreibe in JSON in Datenset Attribute, wie (Key, Grenc_frac)

- Vergleiche Semantic des vorherigen Textes, mit dem Watermarked
    -> Analysiere Unterschiede

- Überlegen was noch möglich
- Präsentation für Mehler

Verbesserungsmöglichkeiten:
    - Watermarking erst nach bestimmter Anzahl an Entropie einbinden
        -> Text bleibt ähnlich zum Original
        -> Watermarking nur einbinden, in Passagen in denen hohe Entropie liegt
